# MIT-Futuremakers

# Responses
7/6 - I'm hoping to learn the fundamentals of AI and apply it to sofware projects focusing on social impact. <br> <br>
7/7 - Our identity isn't individual - we are part of communities. First step to reach our shared goal is crafting a well-thought out narrative (story telling). Need head and heart to execute mindful action successfully (strategy analysis and story motivation). Emotions can affect whether we take our actions (inhibitors or motivators). All good stories have choice, challenge, and outcome. Most have some type of moral or lesson. Story of us (community), self (call to leadership), and now (strategy and action). Pick one or two key choice points that relate to your calling today. Before the leadership session, I thought that my story wasn't worth sharing, but after lisetening to other people's stories and hearing about how well-crafted stories can convey people's stories no matter the persepctive, I realize that everyone's stories need to be heard. <br><br>
7/8 - i.) Supervised learning uses labelled datsets that already have actual results to train the model. Unsupervised learning uses no labelled data during the training process and usually is used to find patterns/relationships among the dataset <br>
ii.) Sklearn does not have the ability to visualize data without additional libraries as sklearn was created for a software engineer's perspecive - focusing on machine learning and data modeling. <br><br>
7/9 - <a href = 'https://www.kaggle.com/adityakadiwal/water-potability'>Dataset</a> <br>
  I would use convonlutional neural networks to develop a solution to detecting clean water because they can be have the benefits of recurrent or feed-forward, while needing fewer weights. They are also faster to count and less prone to overfitting.  <br><br>
7/12 - Tensors are multidimensional arrays that are a combination of components and basic vectors, making them unique. They are used to encode multidimensional data in machine learning. I noticed that the interactive programs I ran in the TensorFlow programs it was able to handle larger datasets and produce more computations in less time.  <br><br>
7/15 - Machine Learning was used to determine which applicants were considered "hireable" material or not for the startup. Using machine learning models and the data of the user selecting recruits, the model mimicked our recruting choices. However, it also brought up human biases propogated in machine leanring, as the model picked up on our unconcious bias of preferring orange over blue people. The model uninteionally learned to amplify our biases, discriminating against qualified blue canidates due to the training set having more ornage people that were hired and possibly the model learning to make an assoication that orange people = more qualified. A real world example of a biased machine learning model is the google image recongition model, where the model may inaccuretly label minorities due to having biased datasets/sample sizes. I would make this model more fair and inclusive by collecting trained datasets that are not similar in distrubtion and collect the data from multiople sources, making sure that the sample sizes for each subgroup are around equal and the data is representative, not prepretuating any stereotypes among the groups. I chose the model because if image recongition continues to provide unfair treatment to certain subgroups, and as technology becomes even more prevalent in our everyday lives, than these people may suffer from the same prejudice explicityly banned in legislations that are propogarted through technology. I intend to avoid algorithmic biases by making sure that my training dataset has a near equal representation of all subgroups and is representative of the range between them. For my evaluation of the model, I'll aslo check to see if my model performs around the same in each subgruop, making sure that one or more subgroups are not being marginilizied. <br><br>
7/16 - CNNs have sparse interaction, meaning that every input does not connect to every output. However, in fully connected NNs, each input is connected to all neurons. CNNs can take in multidimensional (3d) inputs like images, while fully connected NNs can not without modifications to the input. CNNs also have shared parameters, meaning that for getting output, weights applied to one input are the same as the weights applied to all other inputs. Unlike CNNs, fully connected NNs use each element of the weight matrix once and do not revisit it. A CNN is typically composed of convolutional layers, activation layers, pooling layers, flattening layer, and fully connected layers. A convolutional layer is a feature extraction layer returning a 2d feature map. Activation layers apply nonlinearity to the outputs from the previous layer and used to learn more complex features. Pooling layers decrease the size of the feature map and extract only the most important features. Flattening layers convert the 2d feature maps into single linear vector for the fully connected layer. Fully connected layers utualize all features from the previous layers to classify, predict, and return an output. Fully connceted NNs have activation layers that are fully connected to each other and return an output at the last layer. They are composed of an input, activation, and output layer. CNNs can be used for image recongition and classification. Fully connected NNs can be used for more generalized purposes, including regression and classification of stock prices, objects, and grade predictions. 
  
