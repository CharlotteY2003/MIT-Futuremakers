{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News Headline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTVMOUoUkGS/YCKjCS+M1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlotteY2003/MIT-Futuremakers/blob/main/News_Headline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXBwvS34gflZ"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtIG6jhgrb1"
      },
      "source": [
        "#for google colab python is not run into computer so can't access through computer - need to upload files to colab\n",
        "\n",
        "def parse_data(file):\n",
        "  for l in open(file, 'r'):\n",
        "    yield json.loads(l)\n",
        "\n",
        "url = 'Sarcasm_Headlines_Dataset.json'\n",
        "\n",
        "dataset = list(parse_data(url))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLQvymuYhSP-"
      },
      "source": [
        "dataset = pd.DataFrame(data=dataset)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C16x9u7BAb2_"
      },
      "source": [
        "sar_det = dataset[dataset.is_sarcastic==1]\n",
        "sar_det.reset_index(drop=True, inplace=True)\n",
        "acc_det = dataset[dataset.is_sarcastic == 0]\n",
        "acc_det.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O90ndUXxBdi3"
      },
      "source": [
        "def tokenization(data):\n",
        "  arr = []\n",
        "  for rows in range(0, data.shape[0]):\n",
        "    head_txt = data.headline[rows]\n",
        "    head_txt = head_txt.split(\" \")\n",
        "    arr.append(head_txt)\n",
        "  \n",
        "  data_list = list(itertools.chain(*arr))\n",
        "  return data_list, arr"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qi2a_r6CO-7"
      },
      "source": [
        "sar_list, sar_news = tokenization(sar_det)\n",
        "acc_list, acc_news = tokenization(acc_det)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMFB5R3gCfhC",
        "outputId": "db2d60bc-97ec-4745-fc73-f54dc6428822"
      },
      "source": [
        "#Stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "sar_list_restp = [word for word in sar_list if word.lower() not in stopwords]\n",
        "acc_list_restp = [word for word in acc_list if word.lower() not in stopwords]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLBtrIGNEuKS",
        "outputId": "b74028da-ccc8-4e80-c783-d313c123b423"
      },
      "source": [
        "print(\"Length of original sarcasm list: {0} words \\n\"\n",
        "      \"Length of new sarcasm list: {1} words\\n\".format(len(sar_list), len(sar_list_restp)))\n",
        "print('=='*46)\n",
        "print(\"Length of original accuracy list: {0} words \\n\"\n",
        "      \"Length of new accuracy list: {1} words\\n\".format(len(acc_list), len(acc_list_restp)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of original sarcasm list: 115898 words \n",
            "Length of new sarcasm list: 87458 words\n",
            "\n",
            "============================================================================================\n",
            "Length of original accuracy list: 147128 words \n",
            "Length of new accuracy list: 103525 words\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSBD0FI3F5Is"
      },
      "source": [
        "#Stemming - not great because it disregards semantic meaning of word\n",
        "stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords = True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t_vzTwGG34q"
      },
      "source": [
        "#Lemminization\n",
        "lemm = WordNetLemmatizer()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2du0w4uUHM8y"
      },
      "source": [
        "def lemminizor(news, restp):\n",
        "  arr = []\n",
        "  for batch in news:\n",
        "    restp = [word for word in batch if word.lower() not in stopwords]\n",
        "    lemm = WordNetLemmatizer()\n",
        "    list_lemm = [lemm.lemmatize(word) for word in restp]\n",
        "    arr.append(list_lemm)\n",
        "\n",
        "  return arr"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Wa0S1hJe_9"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "sar_wost_lemm = lemminizor(sar_news, sar_list_restp)\n",
        "acc_wost_lemm = lemminizor(acc_news, acc_list_restp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlLrHVtPeC1I"
      },
      "source": [
        "#Actual Start of NLP NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjpaSe3Npa3J"
      },
      "source": [
        "y = dataset.is_sarcastic\n",
        "X = dataset.headline\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = y.reshape(-1,1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiDqOO-eqHxs"
      },
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .33, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iee1EneK3Tm"
      },
      "source": [
        "#max_words = 1000\n",
        "#max_len = 150\n",
        "#tok = Tokenizer(num_words = max_words)\n",
        "#tok.fit_on_texts(X_train)\n",
        "#sequences = tok.texts_to_sequences(X_train)\n",
        "#sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ciORO_qtHo"
      },
      "source": [
        "#model = Sequential()\n",
        "#model.add(Dense(64, activation = 'relu', input_shape = (max_len,)))\n",
        "#model.add(Embedding(max_words, 50, input_length=max_len))\n",
        "#model.add(Dense(64, activation = 'relu'))\n",
        "#model.add(Dropout(.2))\n",
        "#model.add(Dense(1, activation = 'sigmoid'))\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdjIAVOIsCxr"
      },
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n",
        "for train, test in kfold.split(X,y):\n",
        "  max_words = 1000\n",
        "  max_len = 150\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, activation = 'relu', input_shape = (max_len,)))\n",
        "  model.add(Embedding(max_words, 50, input_length=max_len))\n",
        "  model.add(Dense(64, activation = 'relu'))\n",
        "  model.add(Dropout(.2))\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  \n",
        "  tok = Tokenizer(num_words = max_words)\n",
        "  tok.fit_on_texts(X[train])\n",
        "  sequences = tok.texts_to_sequences(X[train])\n",
        "  sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)\n",
        "\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer='RMSprop', metrics = ['accuracy'])\n",
        "  \n",
        "  model.fit(sequences_matrix, y[train], epochs = 5, batch_size = 100, verbose = 1)\n",
        "\n",
        "  test_sequences = tok.texts_to_sequences(X[test])\n",
        "  test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen = max_len)\n",
        "  y_pred = model.predict(test_sequences_matrix)\n",
        "  score = model.evaluate(test_sequences_matrix, y[test])\n",
        "  print(score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
